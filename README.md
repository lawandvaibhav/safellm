# SafeLLM

[![PyPI version](https://badge.fury.io/py/safellm.svg)](https://badge.fury.io/py/safellm)
[![Python Support](https://img.shields.io/pypi/pyversions/safellm.svg)](https://pypi.org/project/safellm/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Type Checked](https://img.shields.io/badge/typed-mypy-blue.svg)](http://mypy-lang.org/)

**Enterprise-grade guardrails and sanitization for LLM applications**

*Deterministic outputs. Safe content. Production-grade controls for AI apps.*

---

## ğŸš€ Quick Start

```python
from safellm import Pipeline, guards

# Create a validation pipeline
pipeline = Pipeline(
    name="content_safety",
    steps=[
        guards.LengthGuard(max_chars=5000),
        guards.PiiRedactionGuard(mode="mask"),
        guards.ProfanityGuard(action="block"),
        guards.HtmlSanitizerGuard(policy="strict"),
    ],
)

# Validate LLM output
llm_output = "Contact me at john@email.com for more info!"

decision = pipeline.validate(llm_output)

if decision.allowed:
    safe_output = decision.output  # "Contact me at j***@email.com for more info!"
    print("âœ… Content is safe:", safe_output)
else:
    print("âŒ Content blocked:", decision.reasons)
```

## ğŸ¯ Key Features

- **ğŸ”’ Content Safety**: PII redaction, profanity filtering, secret masking
- **ğŸ“‹ Schema Validation**: JSON Schema and Pydantic model enforcement  
- **ğŸ§¹ Content Sanitization**: HTML/Markdown cleaning and XSS prevention
- **âš¡ High Performance**: Sub-millisecond validations, async support
- **ğŸ”§ Composable**: Mix and match guards, custom validation pipelines
- **ğŸ“Š Enterprise Ready**: Audit logging, OpenTelemetry integration, privacy controls
- **ğŸŒ Provider Agnostic**: Works with any LLM client (OpenAI, Anthropic, local models)
- **âœ… Battle-Tested**: 75% test coverage with 75+ comprehensive tests across all guard types

## ğŸ“¦ Installation

```bash
# Basic installation
pip install safellm

# Full installation with all optional dependencies
pip install safellm[full]

# Development installation
pip install safellm[dev]
```

## ğŸ›¡ï¸ Guards Available

### Content Safety
- **`LengthGuard`**: Enforce character/token limits
- **`PiiRedactionGuard`**: Detect and redact PII (emails, phones, SSNs, credit cards, etc.)
- **`SecretMaskGuard`**: Mask API keys, tokens, and credentials
- **`ProfanityGuard`**: Filter inappropriate language

### Structure & Format
- **`SchemaGuard`**: Validate against JSON Schema or Pydantic models
- **`HtmlSanitizerGuard`**: Clean HTML content, prevent XSS
- **`MarkdownSanitizerGuard`**: Sanitize Markdown content

### Extensibility
- **Custom Guards**: Implement your own validation logic
- **Policy Engine**: YAML-based rule configuration (coming soon)

## ğŸ“– Examples

### Schema Validation

```python
from pydantic import BaseModel
from safellm import Pipeline, guards

class BlogPost(BaseModel):
    title: str
    content: str
    tags: list[str]

# Validate structured LLM output
pipeline = Pipeline("blog_validation", [
    guards.SchemaGuard.from_model(BlogPost),
    guards.LengthGuard(max_chars=2000),
])

# LLM generates structured data
llm_output = {
    "title": "AI Safety Best Practices",
    "content": "Content about AI safety...",
    "tags": ["ai", "safety", "ml"]
}

decision = pipeline.validate(llm_output)
if decision.allowed:
    validated_post = decision.output  # Type-safe, validated data
```

### PII Protection

```python
pipeline = Pipeline("pii_protection", [
    guards.PiiRedactionGuard(
        mode="mask",
        targets=["email", "phone", "credit_card", "ssn"]
    )
])

sensitive_content = """
Please call me at 555-123-4567 or email john.doe@company.com.
My credit card is 4532-1234-5678-9012.
"""

decision = pipeline.validate(sensitive_content)
print(decision.output)
# Output: "Please call me at ***-***-4567 or email j***@***.com.
#          My credit card is **** **** **** 9012."
```

### Async Usage with Context

```python
import asyncio
from safellm import Pipeline, guards, Context

async def process_llm_output():
    pipeline = Pipeline("async_safety", [
        guards.SecretMaskGuard(),
        guards.HtmlSanitizerGuard(),
    ])
    
    # Add context for audit trails
    ctx = Context(
        model="gpt-4",
        user_role="content_creator",
        purpose="blog_generation"
    )
    
    result = await pipeline.avalidate(content, ctx=ctx)
    return result

# Run async validation
decision = asyncio.run(process_llm_output())
```

## ğŸ›¡ï¸ Available Guards

SafeLLM provides 16+ production-ready guards with comprehensive test coverage:

### Content Safety Guards
- **ProfanityGuard** (95% coverage): Block or mask offensive language
- **ToxicityGuard** (91% coverage): Detect harmful or toxic content
- **LanguageGuard** (94% coverage): Enforce language requirements

### Data Protection Guards  
- **PiiRedactionGuard** (81% coverage): Mask emails, phones, SSNs, credit cards
- **SecretMaskGuard** (68% coverage): Hide API keys, tokens, passwords
- **PrivacyComplianceGuard** (71% coverage): GDPR/CCPA compliance enforcement

### Format & Structure Guards
- **LengthGuard** (100% coverage): Enforce character/token limits
- **FormatGuard** (53% coverage): Validate emails, JSON, custom patterns
- **JsonSchemaGuard** (47% coverage): Enforce JSON Schema compliance
- **HtmlSanitizerGuard** (85% coverage): Clean HTML, prevent XSS

### Business Logic Guards
- **BusinessRulesGuard** (61% coverage): Custom domain-specific validation
- **SimilarityGuard** (74% coverage): Detect duplicate or similar content
- **PromptInjectionGuard** (81% coverage): Prevent prompt injection attacks
- **RateLimitGuard** (79% coverage): Control request frequency

*All guards support async operations and provide detailed evidence for audit trails.*

## ğŸ—ï¸ Architecture

SafeLLM uses a pipeline architecture where data flows through a series of guards:

```
Input Data â†’ Guard 1 â†’ Guard 2 â†’ Guard 3 â†’ Validated Output
    â†“          â†“         â†“         â†“
 Context â†’ Decision â†’ Decision â†’ Decision â†’ Final Decision
```

Each guard can:
- **Allow**: Data passes validation
- **Deny**: Data is rejected with reasons
- **Transform**: Data is modified (e.g., PII redacted)
- **Retry**: Request should be retried (e.g., unclear content)

## ğŸ”§ Configuration

### Environment Variables

```bash
# Telemetry settings
SAFELLM_TELEMETRY=off|basic|otlp

# Default behavior
SAFELLM_DEFAULT_DENY=true|false
SAFELLM_REDACTION_STYLE=mask|remove
```

### Configuration File

Add to your `pyproject.toml`:

```toml
[tool.safellm]
default_deny = true
telemetry = "basic"
redaction_style = "mask"
```

## ğŸ“Š Observability

SafeLLM provides comprehensive observability out of the box:

### Structured Logging
```python
import logging
logging.basicConfig(level=logging.INFO)

# Logs include audit_id, guard names, and sanitized evidence
```

### OpenTelemetry Integration
```python
# Automatic metrics and tracing
pip install safellm[otel]

# Metrics available:
# - safellm.guards.duration (histogram)  
# - safellm.decisions.total (counter)
# - safellm.violations.count (counter)
```

### Audit Events
```python
decision = pipeline.validate(data)
print(f"Audit ID: {decision.audit_id}")
print(f"Evidence: {decision.evidence}")  # Redacted by default
```

## ğŸ”’ Security & Privacy

- **Privacy First**: No data sent to external services by default
- **Redaction Before Logging**: PII is masked in all logs and audit trails
- **Data Minimization**: Configurable retention policies
- **Supply Chain Security**: Signed releases, SBOM generation (roadmap)

## ğŸ§ª Testing & Quality Assurance

SafeLLM maintains **75% test coverage** with comprehensive test suites ensuring reliability and safety:

```bash
# Run all tests (75 comprehensive tests)
python -m unittest discover tests -v

# Generate coverage report (75% coverage achieved)
python -m coverage run -m unittest discover tests
python -m coverage report
python -m coverage html  # View detailed HTML report

# Run specific test categories
python -m unittest tests.unit.test_decisions -v           # Core decision logic
python -m unittest tests.test_comprehensive_coverage -v   # Guard functionality  
python -m unittest tests.test_extended_guards -v          # Advanced guard features
python -m unittest tests.test_specific_coverage -v        # Pipeline & edge cases
```

### Test Coverage Highlights
- **Perfect Coverage (100%)**: Core modules (`context.py`, `decisions.py`, `guards/__init__.py`)
- **Excellent Coverage (90%+)**: Language detection (94%), profanity filtering (95%), toxicity detection (91%)
- **Good Coverage (80%+)**: HTML sanitization (85%), injection prevention (81%), PII redaction (81%), pipeline orchestration (84%)
- **Comprehensive Guard Testing**: All 16+ guard types tested with real-world scenarios
- **Error Handling**: Complete pipeline error handling and recovery testing
- **Async Support**: Full async validation pipeline testing

## ğŸ› ï¸ Development

```bash
# Clone the repository
git clone https://github.com/safellm/safellm
cd safellm

# Install development dependencies
pip install -e .[dev]

# Run comprehensive test suite
python -m unittest discover tests -v

# Generate test coverage report (current: 75%)
python -m coverage run -m unittest discover tests
python -m coverage report
python -m coverage html  # Open htmlcov/index.html for detailed view

# Run pre-commit hooks
pre-commit install

# Run linting and type checking
ruff check src/ tests/
mypy src/safellm
```

### Local CI Checks

SafeLLM includes comprehensive scripts to run all CI checks locally, helping you catch issues before pushing to GitHub:

```bash
# Run all CI checks (recommended before pushing)
python scripts/run_ci_checks.py

# Quick check during development (skips slower checks)
python scripts/run_ci_checks.py --fast

# Skip specific checks
python scripts/run_ci_checks.py --skip mypy,bandit

# Platform-specific alternatives
./scripts/run_ci_checks.sh --fast          # Bash (Unix/Linux/macOS)
.\scripts\run_ci_checks.ps1 -Fast          # PowerShell (Windows)
```

The CI scripts run:
- âœ… **Ruff** - Code linting and style checking
- âœ… **Black** - Code formatting verification  
- âœ… **MyPy** - Static type checking
- âœ… **Bandit** - Security vulnerability scanning
- âœ… **Pytest** - Unit tests with coverage reporting
- âœ… **Build** - Package building and installation test

See [`scripts/README.md`](scripts/README.md) for detailed documentation.

### Testing Framework
- **Comprehensive Coverage**: 75+ tests covering all guard types and edge cases
- **Guard Interface Testing**: Validates proper parameter structures and error handling
- **Pipeline Testing**: Complete validation pipeline behavior including fail-fast and error recovery
- **Async Testing**: Full async validation support with proper error propagation
- **Mock Testing**: Controlled testing environments for error conditions and edge cases

## ğŸ—ºï¸ Roadmap

### Version 0.1.x (Current - Testing & Quality Focus) âœ…
- [x] **75% Test Coverage Achievement**: Comprehensive test suite with 75+ tests
- [x] **All Guard Types Tested**: Complete coverage of 16+ guard implementations
- [x] **Pipeline Error Handling**: Robust error recovery and fail-fast behavior testing
- [x] **Async Validation Support**: Full async pipeline testing infrastructure
- [x] **Edge Case Coverage**: Comprehensive testing of boundary conditions and error states

### Version 0.2.x
- [ ] Policy DSL (YAML-based rules)
- [ ] CLI tools for testing pipelines
- [ ] FastAPI/Flask middleware examples
- [ ] Advanced PII detection (spaCy/Presidio integration)
- [ ] **Target: 85% Test Coverage**

### Version 0.3.x
- [ ] External moderation APIs (OpenAI, Azure, AWS Comprehend)
- [ ] Cost and token tracking
- [ ] Custom transform plugins
- [ ] Performance optimizations
- [ ] **Property-based testing framework**

### Version 1.0.0
- [ ] Stability guarantees
- [ ] Comprehensive documentation
- [ ] Production hardening
- [ ] Signed releases
- [ ] **90%+ Test Coverage with full CI/CD**

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

- ğŸ› [Report bugs](https://github.com/safellm/safellm/issues)
- ğŸ’¡ [Request features](https://github.com/safellm/safellm/issues)
- ğŸ“– [Improve documentation](https://github.com/safellm/safellm/pulls)
- ğŸ”§ [Submit code changes](https://github.com/safellm/safellm/pulls)

## ğŸ“„ License

SafeLLM is licensed under the [Apache License 2.0](LICENSE).

## ğŸ†˜ Support

- ğŸ“š [Documentation](https://safellm.readthedocs.io)
- ğŸ’¬ [GitHub Discussions](https://github.com/safellm/safellm/discussions)
- ğŸ› [Issue Tracker](https://github.com/safellm/safellm/issues)
- ğŸ“§ [Security Issues](mailto:security@safellm.dev)

---

**Made with â¤ï¸ for the AI community**

*SafeLLM helps you ship AI applications with confidence.*
Enterprise-grade guardrails &amp; sanitization for LLM apps. Protects against prompt injection, PII leaks, and unsafe outputs. Lightweight, provider-agnostic, and built for compliance, observability, and CI/CD.
